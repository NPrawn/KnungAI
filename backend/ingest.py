import os
import fitz  # PyMuPDF
import shutil
from PIL import Image
import io
import random
import time
from dotenv import load_dotenv
import numpy as np
from google.cloud import vision
from google.api_core.exceptions import GoogleAPICallError, RetryError
from openai import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from tqdm import tqdm


# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ í•„ìš”)
load_dotenv()

#------------------phoenix api------------------------------------------------------------------------
# Phoenix ì„¤ì •
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = "api_key=f3046c156853d98babc:ca2a364"
os.environ["PHOENIX_CLIENT_HEADERS"] = "api_key=f3046c156853d98babc:ca2a364"
os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "https://app.phoenix.arize.com"

from phoenix.otel import register
from openinference.instrumentation.openai import OpenAIInstrumentor
from phoenix.evals import OpenAIModel, QAEvaluator, run_evals
import pandas as pd

# Phoenix ë“±ë¡ ë° ê³„ì¸¡ê¸° ì„¤ì •
tracer_provider = register(project_name="ocr-ingest-pipeline", auto_instrument=True)
OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)

eval_model = OpenAIModel(model="gpt-4o", temperature=0)
qa_evaluator = QAEvaluator(model=eval_model)

# âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜
# def evaluate_response(query, response_text, reference=None):
#     df = pd.DataFrame({
#         "input": [query],
#         "output": [response_text],
#         "reference": [reference or ""]
#     })

#     results = run_evals(df, evaluators=[qa_evaluator], provide_explanation=True)
#     result = results[0]  # í•˜ë‚˜ë§Œ í‰ê°€ë˜ë¯€ë¡œ 0ë²ˆì§¸

#     return {
#     "score": result["score"].item() if hasattr(result["score"], 'item') else result["score"],
#     "explanation": result["explanation"].item() if hasattr(result["explanation"], 'item') else result["explanation"],
#     "input_excerpt": query.replace("\n", " "),
#     "output_excerpt": response_text.replace("\n", " ")
# }

#--------------------------------------------------------------------------------------------------------



# âœ… í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
vision_client = vision.ImageAnnotatorClient()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# âœ… Google Vision OCR í•¨ìˆ˜ (ì´ë¯¸ì§€ ì €ì¥ ì—†ì´ OCRë§Œ)
def ocr_page_google_vision(page, max_retries=20, base_delay = 3):
    # 1. ê³ í•´ìƒë„ ë³€í™˜ (4ë°° í™•ëŒ€)
    zoom_x = 1.0
    zoom_y = 1.0
    mat = fitz.Matrix(zoom_x, zoom_y)
    pix = page.get_pixmap(matrix=mat)
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    # 2. PIL ì´ë¯¸ì§€ â†’ ë°”ì´ë„ˆë¦¬ ë³€í™˜
    img_byte_arr = io.BytesIO()
    img.save(img_byte_arr, format='PNG')
    img_byte_arr = img_byte_arr.getvalue()

    # 3. Vision API ìš”ì²­
    image = vision.Image(content=img_byte_arr)
    retries = 0
    while retries < max_retries:
        try:
            response = vision_client.document_text_detection(image=image, timeout=30)

            if response.error.message:
                raise Exception(f"Google Vision API ì˜¤ë¥˜: {response.error.message}")

            return response.full_text_annotation.text

        except (GoogleAPICallError, RetryError, TimeoutError, Exception) as e:
            print(f"ğŸš¨ Vision API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {retries + 1}/{max_retries}): {e}")
            delay = base_delay * (2 ** retries) + random.uniform(0, 1)
            print(f"â³ {round(delay, 2)}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
            time.sleep(delay)
            retries += 1

    print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì´ í˜ì´ì§€ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.")
    return ""

# âœ… OpenAI LLMìœ¼ë¡œ ë³¸ë¬¸ ì •ë¦¬ í•¨ìˆ˜
def fix_text_with_llm(ocr_text):
    prompt = f"""
ë‹¤ìŒì€ OCRì„ í†µí•´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì´ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ ìˆœì„œê°€ ìì—°ìŠ¤ëŸ½ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
í•˜ì§€ë§Œ **í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ë‚´ìš©, ë‹¨ì–´, ë¬¸ì¥**ì€ **ì ˆëŒ€ ì¶”ê°€, ì‚­ì œ, ìˆ˜ì •í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€**í•´ì•¼ í•©ë‹ˆë‹¤.
ë³¸ë˜ í‘œ í˜•íƒœì˜€ë˜ ë°ì´í„°ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‘œ í˜•íƒœì˜ ì •ë³´ëŠ” ë‹¤ì‹œ í‘œ í˜•íƒœë¡œ ë³µì›í•´ì£¼ì„¸ìš”

**ì˜¤ì§ ë¬¸ì¥ì˜ ìˆœì„œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ë°°ì¹˜**í•˜ì„¸ìš”.  
ê¸€ì, ë‹¨ì–´, ë¬¸ì¥ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ëˆ„ë½ì‹œí‚¤ë©´ ì•ˆ ë©ë‹ˆë‹¤.  
ì¡°ê¸ˆì´ë¼ë„ ë‚´ìš©ì´ ë°”ë€Œì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

[OCR ì¶”ì¶œ í…ìŠ¤íŠ¸]
{ocr_text}

[ìš”êµ¬ì‚¬í•­]
- ë‹¨ì–´, ë¬¸ì¥ì„ ì¶”ê°€, ì‚­ì œ, ìˆ˜ì •í•˜ì§€ ë§ ê²ƒ
- ìˆœì„œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ë°°ì—´í•  ê²ƒ
- ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ 100% ê·¸ëŒ€ë¡œ ìœ ì§€í•  ê²ƒ
- ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ë§ ê²ƒ
- ë„ì–´ì“°ê¸°ë‚˜ ê°„ë‹¨í•œ ì—°ê²° ì •ë„ëŠ” í—ˆìš©

[ê²°ê³¼]
"""
    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë¬¸ì„œ ì •ë¦¬ ì „ë¬¸ê°€ì•¼. ë³¸ë¬¸ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì‚­ì œí•´ì•¼ í•´."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1
    )
    fixed_text = response.choices[0].message.content.strip()

    print("\nğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ (Prompt):", response.usage.prompt_tokens)
    print("ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ (Completion):", response.usage.completion_tokens)
    print("ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ (Total):", response.usage.total_tokens)

    return fixed_text

# âœ… PDF ì½ê³  í…ìŠ¤íŠ¸ ë¶„í• 
def load_and_split_pdfs(pdf_folder):
    texts = []
    evaluation_results = []

    for filename in os.listdir(pdf_folder):
        if filename.endswith(".pdf"):
            doc = fitz.open(os.path.join(pdf_folder, filename))
            for page_num, page in enumerate(doc):
                if page.get_images():  # âœ… í˜ì´ì§€ì— ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´
                    ocr_text = ocr_page_google_vision(page)
                    fixed_text = fix_text_with_llm(ocr_text)
                    # eval_result = evaluate_response(ocr_text, fixed_text)
                    # evaluation_results.append(eval_result)
                    page_text = fixed_text
                else:
                    # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” í˜ì´ì§€ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    page_text = page.get_text()
                    # ğŸ“Œ í…ìŠ¤íŠ¸ í˜ì´ì§€ë„ í‰ê°€ ì¶”ê°€
                    fixed_text = fix_text_with_llm(page_text)
                    # eval_result = evaluate_response(page_text, fixed_text)
                    # evaluation_results.append(eval_result)
                    page_text = fixed_text

                texts.append(page_text)

                # # âœ… í…ŒìŠ¤íŠ¸ìš© ì¶œë ¥ (ì•ë¶€ë¶„ 500ìë§Œ ë³´ê¸°)
                # print(f"\n[ğŸ“„ {filename} - Page {page_num+1}]")
                # print(page_text)  # ê¸¸ë©´ 500ìê¹Œì§€ë§Œ
                # print("-" * 80)
            doc.close()

    # âœ… í…ìŠ¤íŠ¸ ì¡°ê° ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    return splitter.create_documents(texts), evaluation_results

# âœ… í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  FAISSì— ì €ì¥
def embed_and_save(docs, db_path="vectorstore"):
    embeddings = OpenAIEmbeddings()
    if os.path.exists(os.path.join(db_path, "index.faiss")):
        db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        db.add_documents(docs)
    else:
        db = FAISS.from_documents(docs, embeddings)

    db.save_local(db_path)

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    pdf_folder = "data"  # PDF íŒŒì¼ ë„£ì–´ë‘” í´ë”
    after_folder = "data_after"
    docs, evaluation_results = load_and_split_pdfs(pdf_folder)
    embed_and_save(docs)
    print(" ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (vectorstore í´ë” ì €ì¥)")

    for filename in os.listdir(pdf_folder):
        if filename.endswith(".pdf"):
            src = os.path.join(pdf_folder, filename)
            dst = os.path.join(after_folder,filename)
            try:
                shutil.move(src,dst)
            except Exception as e:
                print(f"{filename} ì´ë™ ì‹¤íŒ¨: {e}")
    
    # âœ… ì „ì²´ í‰ê°€ ê²°ê³¼ DataFrameìœ¼ë¡œ ë³€í™˜ í›„ ì¶œë ¥
    # df_result = pd.DataFrame(evaluation_results)
    # print("\nğŸ“‹ ì „ì²´ í‰ê°€ ìš”ì•½ (scoreë§Œ í‘œë¡œ):")
    # print(df_result[["score"]].to_markdown(index=True))
    # print("\nğŸ“‹ ìƒì„¸ ê²°ê³¼ (ê° rowë³„ ì „ì²´ ì„¤ëª…):")
    # for i, row in df_result.iterrows():
    #     print(f"\nğŸ“ [ì‘ë‹µ {i+1}]")
    #     print(f"ğŸ“Š ì •í™•ë„ ì ìˆ˜: {row['score']}")
    #     print(f"ğŸ“¥ ì§ˆë¬¸ ë‚´ìš©:\n{row['input_excerpt']}")
    #     print(f"ğŸ“¤ ëª¨ë¸ ì‘ë‹µ:\n{row['output_excerpt']}")
    #     print(f"ğŸ§  í‰ê°€ ì„¤ëª…:\n{row['explanation']}")
    #     print("-" * 80)