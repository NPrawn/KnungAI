import os
import sqlite3
import json
import numpy as np
import pandas as pd

from fastapi import FastAPI
from fastapi.responses import Response
from pydantic import BaseModel
from dotenv import load_dotenv
from typing import List

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware

from evaluator import evaluate_response

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… FastAPI ì•± ìƒì„±
app = FastAPI(
    title="í•™êµ ê³µì§€ì‚¬í•­ ê¸°ë°˜ RAG ì±—ë´‡ (FAQ ìºì‹œ + FAISS LangChain RAG)"
)

# âœ… CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory="../frontend/build/static"), name="static")

# âœ… SQLite ì—°ê²°
conn = sqlite3.connect("faq_cache.db", check_same_thread=False)
cursor = conn.cursor()

# âœ… í…Œì´ë¸” ìƒì„± (ì§ˆë¬¸ + ë³¸ë¬¸ context + ì„ë² ë”© + ì‚¬ìš©íšŸìˆ˜)
cursor.execute("""
    CREATE TABLE IF NOT EXISTS faq_cache (
        question TEXT PRIMARY KEY,
        context TEXT,
        embedding BLOB,
        count INTEGER DEFAULT 1
    )
""")
conn.commit()

# âœ… Pydantic ëª¨ë¸
class Message(BaseModel):
    type: str
    text: str

class QuestionRequest(BaseModel):
    question: str
    history: List[Message] = []

class AnswerResponse(BaseModel):
    answer: str


# - ì •ë³´ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ë‚´ìš©ì€ ê³µì§€ì‚¬í•­ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
# âœ… LangChain ì„¤ì •
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¹ì‹ ì€ ê³µì£¼ëŒ€í•™êµ ê³µì§€ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ í–‰ì • ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.

- ë°˜ë“œì‹œ ì•„ë˜ ë³¸ë¬¸(context)ì—ì„œë§Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

- ë¬¸ì²´ëŠ” ê³µì‹ì ì´ê³  ê°„ê²°í•˜ë©°, ì œëª© ì—†ëŠ” ì„œìˆ í˜• ë‹¨ë½ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ê°€ëŠ¥í•œ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ë‚´ìš©ì„ êµ¬ì„±í•˜ì„¸ìš”:
1. ì§ˆë¬¸ì˜ í•µì‹¬ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…
2. ê´€ë ¨ ì¡°ê±´ ë° ê¸°ì¤€
3. ì ˆì°¨ë‚˜ ì‹ ì²­ ë°©ë²•
4. í•„ìš”í•œ ê²½ìš° ìœ ì˜ì‚¬í•­
- ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ ê³µì§€ì‚¬í•­ì˜ URLì„ í¬í•¨í•˜ì„¸ìš”:
ì¶œì²˜: 

---

ì§ˆë¬¸: {question}

ê³µì§€ì‚¬í•­ ë³¸ë¬¸ (context):
{context}
"""
)

embeddings = OpenAIEmbeddings()
vectorstore_path = "vectorstore"

# âœ… FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
if os.path.exists(vectorstore_path):
    db = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)
else:
    db = FAISS.from_documents([], embeddings)

retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 3})
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

# âœ… LangChain RAG QA ì²´ì¸
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)

# âœ… ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_embedding(text: str):
    return embeddings.embed_query(text)

# âœ… FAQ DB ê²€ìƒ‰ í•¨ìˆ˜
def find_similar_cached_context(question: str, threshold=0.92):
    question_vector = np.array(get_embedding(question))

    cursor.execute("""
    SELECT question, context, embedding 
    FROM faq_cache
    ORDER BY count DESC
    LIMIT 2""")

    rows = cursor.fetchall()

    best_score = 0
    best_question = None
    best_context = None

    for db_question, db_context, db_embedding_blob in rows:
        if db_embedding_blob is None:
            continue

        db_vector = np.frombuffer(db_embedding_blob, dtype=np.float32)
        cosine_sim = np.dot(question_vector, db_vector) / (np.linalg.norm(question_vector) * np.linalg.norm(db_vector))

        if cosine_sim > best_score:
            best_score = cosine_sim
            best_question = db_question
            best_context = db_context

    if best_score >= threshold and best_context:
        cursor.execute("UPDATE faq_cache SET count = count + 1 WHERE question = ?", (best_question,))
        conn.commit()
        return best_context
    else:
        return None

# âœ… ìƒˆ ì§ˆë¬¸ + ë³¸ë¬¸ context ì €ì¥ í•¨ìˆ˜
def save_question_context(question: str, context: str):
    embedding = np.array(get_embedding(question), dtype=np.float32).tobytes()
    cursor.execute(
        "INSERT OR IGNORE INTO faq_cache (question, context, embedding) VALUES (?, ?, ?)",
        (question, context, embedding)
    )
    conn.commit()

#íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ question ìš”ì•½
def summarize_question(history: List[dict], current_question: str) -> str:
    history_str = ""
    for msg in history:
        if msg["type"] == "user":
            history_str += f"ì‚¬ìš©ì: {msg['text']}\n"
        elif msg["tpye"] == "bot":
            history_str += f"í¬ëˆ™ì´: {msg['text']}\n"

        prompt = f"""
ë„ˆëŠ” â€œí•™ìƒ ì§ˆë¬¸ ìš”ì•½ ë¹„ì„œâ€ì•¼.
ë„ˆì˜ ì—­í• ì€ í•™ìƒì´ ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ì§ˆë¬¸ì„, ì§ì „ ëŒ€í™” íë¦„ì„ ê³ ë ¤í•´ì„œ ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ëŠ” ê²ƒì´ì•¼.

â¸»

ğŸ§  ì§€ì¼œì•¼ í•  ê·œì¹™
	1.	ë°˜ë“œì‹œ ë§ˆì§€ë§‰ ì§ˆë¬¸(current_question)ì˜ ì˜ë¯¸ë§Œ ì¬êµ¬ì„±í•´.
    â†’ ë¬¸ì¥ í˜•íƒœëŠ” ë°”ê¿”ë„ ë˜ì§€ë§Œ, ìƒˆë¡œìš´ ì •ë³´(ì˜ˆ: ìê²© ìš”ê±´, ì¡°ê±´ ë“±)ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆ.
	2.	ëŒ€í™” ë§¥ë½(history_str)ì€ ì°¸ê³ ë§Œ í•˜ê³ , í•µì‹¬ì€ ë§ˆì§€ë§‰ ì§ˆë¬¸ì—ì„œ íŒŒì•…í•´.
	3.	í•™ìƒ ë§íˆ¬ë¡œ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ë¬»ëŠ” ì§ˆë¬¸í˜• ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•´.

ë‹¤ìŒì€ ì§€ê¸ˆ ê¹Œì§€ì˜ ëŒ€í™”ì•¼:
{history_str}
ë§ˆì§€ë§‰ ì§ˆë¬¸:
{current_question}


        """
        llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
        response = llm.invoke(prompt)
        return response.content.strip()

# âœ… ì§ˆë¬¸ ì²˜ë¦¬ API
@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    try:
        last_question = request.question.strip()
        history = [msg.model_dump() for msg in request.history]

        question = summarize_question(history, last_question)

        # 1. FAQ ìºì‹œì—ì„œ ë¨¼ì € ë¹„ìŠ·í•œ ì§ˆë¬¸ ì°¾ê¸°
        context = False #find_similar_cached_context(question)

        # if context:
        #     # âœ… context ê¸°ë°˜ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±
        #     prompt = custom_prompt.format(context=context, question=question)
        #     response = llm.invoke(prompt)
        #     #evaluate---------------------------------------------------------------------------------------------------------------
        #     eval_result = evaluate_response(question, response.content, reference=context)
        #     with open("evaluation_log.txt", "a", encoding="utf-8") as f:
        #         f.wirte(f"\n\n[ì§ˆë¬¸] {question}")
        #         f.write(f"\n[ë‹µë³€] {response.content}")
        #         f.write(f"\n[context] {context}")

        #         for result in eval_result:
        #             f.write(f"\ní‰ê°€ì§€í‘œ: {result['evaluator']}")
        #             f.write(f"\nì ìˆ˜: {result['score']}")
        #             f.write(f"\nì„¤ëª…: {result['explanation']}")
        #         f.write(f"{'-----' * 20}\n")
        #     #-----------------------------------------------------------------------------------------------------------------------

        #     return AnswerResponse(answer=response.content)

        # 2. ëª» ì°¾ìœ¼ë©´ FAISSë¥¼ LangChain QAë¡œ ê²€ìƒ‰
        #    (qa_chainì´ retriever.get_relevant_documents() + LLM í˜¸ì¶œì„ ê°™ì´ í•´ì¤€ë‹¤)
        relevant_docs = retriever.get_relevant_documents(question)

        if not relevant_docs:
            return AnswerResponse(answer="ê´€ë ¨ëœ ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # âœ… ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ í•©ì³ì„œ í•˜ë‚˜ì˜ context ë§Œë“¤ê¸°
        context = "\n".join([doc.page_content for doc in relevant_docs])

        # âœ… ë§Œë“  contextë¡œ LLMì— ì§ˆë¬¸í•´ì„œ ë‹µë³€ ìƒì„±
        prompt = custom_prompt.format(context=context, question=question)
        response = llm.invoke(prompt)
        #evaluate---------------------------------------------------------------------------------------------------------------
        eval_result = evaluate_response(question, response.content, reference=context)
        with open("evaluation_log.txt", "a", encoding="utf-8") as f:
            f.write(f"\n\n[ì§ˆë¬¸] {question}")
            f.write(f"\n[ë‹µë³€] {response.content}")
            f.write(f"\n[context] {context}")

            f.write("\n")
            f.write(eval_result[['label', 'score']].to_string(index=False))
            f.write('\n\n')
            for label, explanation in zip(eval_result["label"], eval_result["explanation"]):
                f.write(f"[{label}] \n{explanation}\n")
            f.write(f"\n{'-----' * 20}\n")
        #-----------------------------------------------------------------------------------------------------------------------
        

        # âœ… ì§ˆë¬¸ + FAISS ê²€ìƒ‰ëœ contextë¥¼ FAQ DBì— ì €ì¥
        # save_question_context(question, context)

        return AnswerResponse(answer=response.content)

    except Exception as e:
        print(f"â—ì—ëŸ¬ ë°œìƒ: {e}")
        return Response(
            content=json.dumps({"error": str(e)}, ensure_ascii=False),
            media_type="application/json; charset=utf-8",
            status_code=500
        )

# ë£¨íŠ¸
# React ì •ì  ë¹Œë“œ ì „ì²´ë¥¼ ë£¨íŠ¸ì—ì„œ ì„œë¹™
app.mount("/", StaticFiles(directory="../frontend/build", html=True), name="frontend")